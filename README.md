# Drohnen-RL Environment ğŸš

Ein Gymnasium-kompatibles Reinforcement Learning Environment fÃ¼r Quadcopter-Steuerung.

## Features

- **Realistische Physik**: Vereinfachte Quadcopter-Physik mit 4 unabhÃ¤ngigen Motoren in X-Konfiguration
- **Dynamischer Wind**: Ornstein-Uhlenbeck-Prozess fÃ¼r realistische WindÃ¤nderungen
- **Dense Reward**: `1/(1 + distance)` fÃ¼r effizientes Training
- **Visualisierung**: 2D Top-Down-Ansicht mit matplotlib
- **Gymnasium-kompatibel**: Standard RL-Interface fÃ¼r einfache Integration

## Installation

### Einfache Installation
```bash
pip install -r requirements.txt
```

### Entwickler-Installation (empfohlen)
```bash
pip install -e .
```

### Mit RL-Training-UnterstÃ¼tzung
```bash
pip install -e ".[rl]"
```

## Schnellstart

### Basis-Test
```python
from src.drone_env import DroneEnv

env = DroneEnv(max_steps=1000, render_mode="human")
obs, info = env.reset()

for _ in range(1000):
    action = env.action_space.sample()  # ZufÃ¤llige Aktion
    obs, reward, terminated, truncated, info = env.step(action)
    env.render()
    
    if terminated or truncated:
        break

env.close()
```

### Tests ausfÃ¼hren
```bash
# Alle Tests
python tests/test_env.py

# Rendering-Test
python tests/test_rendering.py

# Minimaler Test
python tests/test_minimal_render.py

# Debug-Informationen
python tests/debug_render.py
```

### Beispiele ausfÃ¼hren
```bash
# Random Agent (ohne Visualisierung)
python examples/random_agent.py --episodes 5 --steps 500

# Random Agent (mit Visualisierung)
python examples/random_agent.py --episodes 3 --steps 500 --render

# Hover Agent (Baseline)
python examples/random_agent.py --agent hover --episodes 3 --render

# RL-Training (benÃ¶tigt stable-baselines3)
python examples/training.py --mode train --algorithm PPO --timesteps 100000

# Modell evaluieren
python examples/training.py --mode eval --model-path models/drone_model
```

## Projektstruktur

```
drone-control/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ drone_env/
â”‚       â”œâ”€â”€ __init__.py          # Package-Initialisierung
â”‚       â””â”€â”€ env.py               # DroneEnv Klasse
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_env.py              # Umfassende Tests
â”‚   â”œâ”€â”€ test_rendering.py        # Rendering-Tests
â”‚   â”œâ”€â”€ test_minimal_render.py   # Minimaler Rendering-Test
â”‚   â””â”€â”€ debug_render.py          # Debug-Informationen
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ random_agent.py          # Random/Hover Agent Demo
â”‚   â””â”€â”€ training.py              # SB3 Training & Evaluation
â”‚
â”œâ”€â”€ setup.py                     # Package-Setup
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ README.md                    # Diese Datei
â””â”€â”€ .gitignore                   # Git Ignore
```

## Environment Details

### Action Space
- **Typ**: `Box(4,)` 
- **Wertebereich**: [0, 1] pro Motor (0% - 100% Thrust)
- **Beschreibung**: 
  - Motor 0: Vorne-Rechts
  - Motor 1: Hinten-Links
  - Motor 2: Vorne-Links
  - Motor 3: Hinten-Rechts

### Observation Space
- **Typ**: `Box(15,)`
- **Komponenten**:
  - `[0:3]` - Position relativ zum Ziel (x, y, z)
  - `[3:6]` - Lineare Geschwindigkeit (vx, vy, vz)
  - `[6:9]` - Orientierung (Roll, Pitch, Yaw in Radiant)
  - `[9:12]` - Winkelgeschwindigkeit (wx, wy, wz)
  - `[12:15]` - Windvektor absolut (wx, wy, wz)

**Hinweis**: Die Drohne ist in der Beobachtung immer bei Position (0, 0, 0). Der Zielpunkt wird relativ angegeben.

### Reward
```python
reward = 1.0 / (1.0 + distance_to_target)
```
- **Wertebereich**: (0, 1]
- **Maximum**: 1.0 (Drohne am Ziel)
- **Eigenschaften**: Dense, kontinuierlich, differenzierbar

### Termination
- **Terminated**: Immer `False` (keine Crash-Detektion)
- **Truncated**: `True` nach `max_steps` Schritten
- **Standard**: 1000 Steps (~10 Sekunden bei 100 Hz)

## Physik-Modell

### Quadcopter-Konfiguration
- **X-Konfiguration**: Rotoren diagonal angeordnet (Â±45Â° zu Achsen)
- **Masse**: 1.0 kg
- **Arm-LÃ¤nge**: 0.25 m
- **TrÃ¤gheitsmomente**: [0.01, 0.01, 0.02] kgÂ·mÂ²

### Kraftberechnung
1. **Thrust**: Kraftvektor senkrecht zur Rotorebene, skaliert mit Motor-Power
2. **Drehmoment**:
   - **Roll**: Thrust-Differenz zwischen linken/rechten Motoren
   - **Pitch**: Thrust-Differenz zwischen vorderen/hinteren Motoren
   - **Yaw**: Reaktives Drehmoment aus Rotor-Drehrichtungen
3. **Wind**: Kraftvektor proportional zur Windgeschwindigkeit
4. **Gravitation**: -9.81 m/sÂ² in Z-Richtung

### Integration
- **Methode**: Euler-Integration
- **Zeitschritt**: 0.01 s (100 Hz Standard)

## Konfigurationsparameter

```python
from src.drone_env import DroneEnv

env = DroneEnv(
    max_steps=1000,                          # Episode-LÃ¤nge
    dt=0.01,                                 # Zeitschritt in Sekunden
    target_change_interval=None,             # Ziel-Ã„nderung (None = fix)
    wind_strength_range=(0.0, 5.0),         # Wind-StÃ¤rke in m/s
    render_mode="human"                      # "human", "rgb_array", oder None
)
```

## Entwicklung

### Paket installieren (editable mode)
```bash
pip install -e .
```

### Tests ausfÃ¼hren
```bash
# Alle Tests
python tests/test_env.py

# Spezifischer Test
python tests/test_minimal_render.py
```

### Code formatieren (optional)
```bash
pip install -e ".[dev]"
black src/ tests/ examples/
flake8 src/ tests/ examples/
```

## ZukÃ¼nftige Erweiterungen

- [ ] 3D-Visualisierung (PyVista oder Pygame)
- [ ] Optionale Windrichtung aus Observation Space entfernen (fÃ¼r recurrent policies)
- [ ] Mehrere Zielpunkte pro Episode
- [ ] Hindernisse
- [ ] Crash-Detektion und -Penalties
- [ ] Energieverbrauch als Teil der Reward-Funktion
- [ ] Aerodynamische Effekte (Luftwiderstand, Downwash)

## Trainings-Tipps

### Baseline
Ein einfacher Hover-Agent (alle Motoren auf ~25%) erreicht einen durchschnittlichen Reward von ~0.05-0.10, abhÃ¤ngig von der initialen Ziel-Distanz.

### Empfohlene Algorithmen
- **SAC** (Soft Actor-Critic): Gut fÃ¼r kontinuierliche Actions
- **PPO** (Proximal Policy Optimization): Stabil und sample-effizient
- **TD3** (Twin Delayed DDPG): FÃ¼r deterministische Policies

### Recurrent Policies
Um die Drohne Wind "spÃ¼ren" zu lassen:
1. Entferne `wind_vector` aus Observation Space (Index 12:15)
2. Verwende LSTM/GRU-basierte Policy
3. Drohne muss Wind aus Positions-/Geschwindigkeits-Historie inferieren

## Lizenz

MIT

## Autor

Adrian - 2025

